{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"collapsed":false},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","pd.set_option(\"display.show_dimensions\", False)\n","pd.set_option(\"display.float_format\", \"{:4.2g}\".format)"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["from IPython.core.magic import register_line_magic\n","\n","@register_line_magic\n","def C(line):\n","    from IPython.core.getipython import get_ipython\n","    from fnmatch import fnmatch\n","\n","    line = line.strip()\n","    if ' ' in line:\n","        idx_space = line.index(' ')\n","        space_num = line[:idx_space]\n","        if space_num.isdecimal():\n","            space_num = int(space_num)\n","            line = line[idx_space:]\n","        else:\n","            space_num = 5\n","    else:\n","        space_num = 5\n","\n","    output_dict = {}\n","    cmds = line.split(';')\n","    for cmd in cmds:\n","        cmd = cmd.strip()\n","        if cmd != \"\":\n","            output_dict[cmd] = repr(eval(cmd)).split(\"\\n\")\n","\n","    str_maxlen_in_cols = [max(len(cmd), len(max(data, key=len))) for cmd, data in output_dict.items()]\n","    data_row_max = max([len(v) for v in output_dict.values()])\n","\n","    out_lines = [\"\"]*(data_row_max+2)\n","\n","    space=''\n","    for i, (cmd, data) in enumerate(output_dict.items()):\n","        w = str_maxlen_in_cols[i]\n","\n","        out_lines[0]+=space+f'{cmd:^{w}}'\n","        out_lines[1]+=space+\"-\"*w\n","        for j, d in enumerate(data, 2):\n","            out_lines[j]+=space+f'{d:{w}}'\n","\n","        if len(data) < data_row_max:\n","            for j in range(len(data)+2, data_row_max+2):\n","                out_lines[j]+=space+' '*w\n","        \n","        space = ' '*space_num\n","\n","    for line in out_lines:\n","        print(line)"]},{"cell_type":"markdown","metadata":{},"source":["## 檔案的輸入輸出\n","\n","表 5-3 輸入輸出函數\n","|函數名稱 |說明 |\n","|--------|-----|\n","|read_csv() |從 CSV 格式的文字檔讀取資料 |\n","|read_excel() |從 Excel 檔案讀取資料 |\n","|HDFStore() |使用 HDF5 檔案讀寫資料 |\n","|read_sql() |從 SQL 資料庫的查詢結果載入資料 |\n","|read_pickle() |讀取 Pickle 序列化之後的資料 |"]},{"cell_type":"markdown","metadata":{},"source":["### CSV檔案\n","\n","`read_csv()` 從文字檔讀取資料，它的可選參數非常多，下面只簡介一些常用的參數：\n","- `sep` : 指定資料的分隔符號，可以使用正規表示法，預設值為逗號。有時 CSV 檔案為了便於閱讀，在逗號之後增加了一些空格以對齊每列的資料。如果希望忽略這些空格，可以將 `skipinitialspace` 參數設定為 `True`。\n","- 如果資料使用 空格 或 定位字符 分隔，可以不設定 `sep` 參數，而將 `delim_whitespace` 參數設定為 `0`。\n","- 預設情況下第一行文字被作為列索引標籤，如果資料檔案中沒有儲存列名稱的行，可以設定 `header` 參數為 `0`。\n","- 如果資料檔案之前包含一些說明行，可以使用 `skiprows` 參數指定資料開始的行號。\n","- `na_values`, `true_values`, `false_values` 等參數分別指定 `NaN`, `True`, `False` 對應的字串清單。\n","- 如果希望從字串讀取資料，可以使用 `io.BytesIO(string)` 將字串包裝成輸入流。\n","- 如果希望將字串轉為 時間，可以使用 `parse_dates` 指定轉為時間的列。\n","- 如果資料中包含中文，可以使用 `encoding` 參數指定檔案的編碼，例如 \"utf-8\", \"gbk\" 等。指定編碼之後獲得的字串列為 Unicode 字串。\n","- 可以使用 `usecols` 參數指定需要讀取的列。\n","- 當檔案很大時，可以用 `chunksize` 參數指定一次讀取的行數。當使用 `chunksize` 時，`read_csv()` 傳回一個反覆運算器。\n","- 當檔案名稱包含中文時，需要使用 Unicode 字串指定檔案名稱。\n","\n","下面使用上面介紹的各個參數讀取上海市的空氣品質資料檔案。該檔案的文字編碼為 UTF-8，並且帶 BOM 。所謂 BOM ，是指在檔案開頭的 3 個特殊位元組表示該檔案為 UTF-8 檔案。對於帶 BOM 的 UTF-8 檔案，可以指定編碼參數 `encoding` 為 \"utf-8-sig\"。\n","\n","該檔案中有兩種字元表示缺失資料：一個是減號，另一個是全形的橫杠。由於 `read_csv()` 在將位元組字串轉為 Unicode 之前判斷 `NaN` ，因此需要使用與檔案相同的編碼表示這些缺失資料的字串。"]},{"cell_type":"markdown","metadata":{},"source":["> **LINK**\n","\n","> http://air.epmap.org/\n","\n","> 空氣質量資料來源：青悅空氣質量歷史資料庫"]},{"cell_type":"code","execution_count":3,"metadata":{"collapsed":false},"outputs":[{"name":"stdout","output_type":"stream","text":["df_list[0].count()        df_list[0].dtypes   \n","------------------     -----------------------\n","時間       100           時間       datetime64[ns]\n","監測點       90           監測點              object\n","AQI      100           AQI               int64\n","PM2.5    100           PM2.5             int64\n","PM10      98           PM10            float64\n","dtype: int64           dtype: object          \n"]}],"source":["df_list = []\n","\n","for df in pd.read_csv(\n","        u\"data/aqi/上海市_201406.csv\", \n","        encoding=\"utf-8-sig\",  #檔案解碼\n","        chunksize=100,         #一次讀入的行數\n","        usecols=[u\"時間\", u\"監測點\", \"AQI\", \"PM2.5\", \"PM10\"], #只讀入這些列\n","        na_values=[\"-\", \"—\"],  #這些字串表示缺失資料\n","        parse_dates=[0]):      #第一列為時間列\n","    df_list.append(df)  #在這裡處理資料\n","\n","%C df_list[0].count(); df_list[0].dtypes"]},{"cell_type":"markdown","metadata":{},"source":["注意：「時間」列為 `datetime64[ns]` 型態，而由於存在缺失資料，因此 \"PM10\" 列被轉為浮點數型態，其他的數值列為整數型態，而「監測點」列中儲存的是 Unicode 字串。"]},{"cell_type":"code","execution_count":4,"metadata":{"collapsed":false},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'str'>\n"]}],"source":["# print( type(df.loc[0, u\"監測點\"]) )\n","print( type(df[u\"監測點\"].iloc[0]) )"]},{"cell_type":"markdown","metadata":{},"source":["### HDF5檔案"]},{"cell_type":"markdown","metadata":{},"source":["> **LINK**\n","\n","> http://www.nsmc.cma.gov.cn/FENGYUNCast/docs/HDF5.0_chinese.pdf\n","\n","> 中文的HDF5使用簡介"]},{"cell_type":"markdown","metadata":{},"source":["HDF5 是儲存科學計算資料的一種檔案格式，支援大於 2GB 的檔案，可以把它看作針對科學計算的資料庫檔案。關於 HDF5 檔案格式的更多資訊，請參考上面的連結：(2022-01-19 連結己失效)\n","\n","HDF5 檔案像一個儲存資料的檔案系統，其中只有兩種型態的物件：資料資料(dataset) 和 目錄(group)：\n","\n","- 資料資料(dataset)：像檔案系統中的檔案一樣用於儲存各種資料，例如 numpy 陣列。\n","- 目錄(group)：類似檔案系統中的資料夾，可以包含其他的 目錄(group) 或 資料資料(dataset)。\n","\n","使用 Pandas 可以很方便地將多個 `Series` 和 `DataFrame` 儲存進 HDF5 檔案。HDF5 檔案採用二進位格式儲存資料，可以對資料進行壓縮儲存，比文字檔案更節省空間，存取也更迅速。\n","\n","下面建立一個 `HDFStore` 物件，透過 `complib` 參數指定使用 \"blosc\" 壓縮資料，透過 `complevel` 參數指定壓縮層級。"]},{"cell_type":"code","execution_count":5,"metadata":{"collapsed":false},"outputs":[],"source":["store = pd.HDFStore(\"a.hdf5\", complib=\"blosc\", complevel=9)"]},{"cell_type":"markdown","metadata":{},"source":["`HDFStore` 物件支援字典介面，例如使用 `[]` 存取元素, `get()` 和 `keys()` 等方法。"]},{"cell_type":"code","execution_count":6,"metadata":{"collapsed":false},"outputs":[{"name":"stdout","output_type":"stream","text":["['/series/s1', '/dataframes/df1', '/dataframes/df2', '/dataframes/df_dynamic1']\n","True\n"]}],"source":["df1 = pd.DataFrame(np.random.rand(100000, 4), columns=list(\"ABCD\"))\n","df2 = pd.DataFrame(np.random.randint(0, 10000, (10000, 3)), \n","                   columns=[\"One\", \"Two\", \"Three\"])\n","s1 = pd.Series(np.random.rand(1000))\n","store[\"dataframes/df1\"] = df1\n","store[\"dataframes/df2\"] = df2\n","store[\"series/s1\"] = s1\n","print( store.keys() )\n","print( df1.equals(store[\"dataframes/df1\"]) )"]},{"cell_type":"markdown","metadata":{},"source":["`HDFStore` 採用 `pytables` 擴充庫存取 HDF5 檔案，其 `get_node()` 方法可以獲得 `pytables` 中定義的 `Node` 物件。使用該物件可以檢查檔案中的所有節點，關於 `Node` 物件的用法，請參考 `pytables` 的文件。"]},{"cell_type":"markdown","metadata":{},"source":["> **LINK**\n","\n","> http://pytables.github.io/usersguide/libref/hierarchy_classes.html\n",">\n","> `pytables`官方文件"]},{"cell_type":"markdown","metadata":{},"source":["下面用 `get_node()` 獲得根節點，然後呼叫 `_f_walknodes()` 檢查其包含的所有節點。由結果可知，`HDFStore` 中的 `Series` 和 `DataFrame` 物件與 HDF5 的目錄對應，目錄中透過多個資料資料(dataset)儲存實際的資料。"]},{"cell_type":"code","execution_count":7,"metadata":{"collapsed":false},"outputs":[{"name":"stdout","output_type":"stream","text":["/dataframes (Group) ''\n","/series (Group) ''\n","/dataframes/df1 (Group) ''\n","/dataframes/df2 (Group) ''\n","/dataframes/df_dynamic1 (Group) ''\n","/series/s1 (Group) ''\n","/series/s1/index (CArray(1000,)shuffle, blosc(9)) ''\n","/series/s1/values (CArray(1000,)shuffle, blosc(9)) ''\n","/dataframes/df1/axis0 (CArray(4,)shuffle, blosc(9)) ''\n","/dataframes/df1/axis1 (CArray(100000,)shuffle, blosc(9)) ''\n","/dataframes/df1/block0_items (CArray(4,)shuffle, blosc(9)) ''\n","/dataframes/df1/block0_values (CArray(100000, 4)shuffle, blosc(9)) ''\n","/dataframes/df2/axis0 (CArray(3,)shuffle, blosc(9)) ''\n","/dataframes/df2/axis1 (CArray(10000,)shuffle, blosc(9)) ''\n","/dataframes/df2/block0_items (CArray(3,)shuffle, blosc(9)) ''\n","/dataframes/df2/block0_values (CArray(10000, 3)shuffle, blosc(9)) ''\n","/dataframes/df_dynamic1/table (Table(100000,)shuffle, blosc(9)) ''\n"]}],"source":["root = store.get_node(\"//\")\n","for node in root._f_walknodes():\n","    print( node )"]},{"cell_type":"markdown","metadata":{},"source":["透過前面介紹的方法將 `DataFrame` 物件儲存進 `HDFStore` 之後，無法再為其追加資料。在資料獲取和匯入大量 CSV 檔案時，我們通常希望能不斷地往同一 `DataFrame` 中增加新的資料。可以使用 `append()` 方法實現該功能。\n","\n","❶ `append` 參數為 `False` 表示將覆蓋己存在的資料，如果指定鍵值不存在，可省略該參數。\n","\n","❷ 將 `df3` 追加到指定鍵，因此讀取該鍵將獲得一個長度為 100100 的 `DataFrame` 物件。"]},{"cell_type":"code","execution_count":8,"metadata":{"collapsed":false},"outputs":[{"data":{"text/plain":["(100100, 4)"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["store.append('dataframes/df_dynamic1', df1, append=False) #❶\n","df3 = pd.DataFrame(np.random.rand(100, 4), columns=list(\"ABCD\"))\n","store.append('dataframes/df_dynamic1', df3) #❷\n","store['dataframes/df_dynamic1'].shape"]},{"cell_type":"markdown","metadata":{},"source":["使用 `append()` 將建立 `pytables` 中支援索引的表格(Table)節點，預設使用 `DataFrame` 的 `index` 作為索引。透過 `select()` 可以對表格進行查詢以取得滿足查詢準則的行(row)。在下面的程式中，透過 `where` 參數指定查詢準則，`index` 表示 `DataFrame` 的標籤資料。該條件取得標籤在 97 到 102 之間的所有行。由於我們將兩個預設標籤的 `DataFrame` 增加進該表格，因此 98 和 99 各對應兩行資料。使用該方式讀取部分資料時，可以減少記憶體使用量和磁碟讀取量，加強資料的存取速度。"]},{"cell_type":"code","execution_count":9,"metadata":{"collapsed":false},"outputs":[{"name":"stdout","output_type":"stream","text":["        A    B     C     D\n","98   0.65 0.93  0.74  0.76\n","99   0.52 0.85 0.062  0.22\n","100  0.44 0.11  0.93  0.24\n","101   0.9 0.51  0.23  0.35\n","98   0.21 0.49 0.069 0.077\n","99  0.045 0.52  0.74  0.99\n"]}],"source":["print( store.select('dataframes/df_dynamic1', where='index > 97 & index < 102') )"]},{"cell_type":"markdown","metadata":{},"source":["如果希望對 `DataFrame` 的指定列進行索引，可以在用 `append()` 建立新的表格時，透過 `data_columns` 指定索引列，或將其設定為 `True` 以對所有列建立索引。"]},{"cell_type":"code","execution_count":10,"metadata":{"collapsed":false},"outputs":[{"name":"stdout","output_type":"stream","text":["         A       B    C     D\n","1342     1   0.003 0.28  0.79\n","6605     1  0.0078 0.77  0.65\n","13779 0.99  0.0041 0.17   0.4\n","14888    1  0.0039 0.11  0.93\n","42852 0.99  0.0043 0.51  0.14\n","54590 0.99  0.0031 0.92     1\n","73805    1  0.0078 0.35  0.65\n","79281 0.99 0.00061 0.83  0.61\n","82027 0.99  0.0075 0.83  0.15\n","95532 0.99   0.003 0.33 0.076\n"]}],"source":["store.append('dataframes/df_dynamic1', df1, append=False, data_columns=[\"A\", \"B\"])\n","print( store.select('dataframes/df_dynamic1', where='A > 0.99 & B < 0.01') )"]},{"cell_type":"markdown","metadata":{},"source":["下面循環讀取 `data\\aqi` 路徑之下的所有 CSV 檔案，並將資料寫入 HDF5 檔案中。在將多個文字檔的資料逐次寫入 HDF5 檔案時，需要注意以下幾點事項：\n","\n","- HDF5 檔案不支援 Unicode 字串，因此需要對 Unicode 字串進行編碼，轉為位元組字串。在本例中直接從檔案讀取 UTF-8 編碼的字串，因此在讀取 CSV 檔案時無須指定 `encoding` 參數。❶ 但是由於檔案可能包含 UTF-8 的 BOM，因此需要先讀取檔案的頭三個位元組並與 BOM 比較，這樣才能保障讀取的資料中與第一列對應的標籤不包含 BOM。\n","- 由於可能存在缺失資料，因此讀取的數值列的型態可能為整數和浮點數。由於 HDF5 檔案中的每列資料只能對應一種型態。❷ 因此需要使用 `dtype` 參數指定這些數值列的型態為浮點數。\n","- ❸ 需要為 HDF5 檔案中的字串列指定最大長度，否則該最大長度將由第一個被增加進 HDF5 檔案的資料的物件決定。"]},{"cell_type":"markdown","metadata":{},"source":["> **WARNING**\n","\n","> 由於所有從CSV檔案讀入`DataFrame`物件的行索引都為預設值，因此HDF5檔案中的資料的行索引並不是唯一的。"]},{"cell_type":"code","execution_count":11,"metadata":{"collapsed":false},"outputs":[],"source":["def read_aqi_files(fn_pattern):\n","    from glob import glob\n","    from os import path\n","    \n","    UTF8_BOM = b\"\\xEF\\xBB\\xBF\"\n","    \n","    cols = \"時間,城市,監測點,質量等級,AQI,PM2.5,PM10,CO,NO2,O3,SO2\".split(\",\")\n","    float_dtypes = {col:float for col in \"AQI,PM2.5,PM10,CO,NO2,O3,SO2\".split(\",\")}\n","    names_map = {\"時間\":\"Time\", \n","                 \"監測點\":\"Position\", \n","                 \"質量等級\":\"Level\", \n","                 \"城市\":\"City\", \n","                 \"PM2.5\":\"PM2_5\"}\n","    \n","    for fn in glob(fn_pattern):\n","        with open(fn, \"rb\") as f:\n","            sig = f.read(3) #❶\n","            if sig != UTF8_BOM:\n","                f.seek(0, 0)\n","            df = pd.read_csv(f, \n","                             parse_dates=[0], \n","                             na_values=[\"-\", \"—\"], \n","                             usecols=cols, \n","                             dtype=float_dtypes) #❷\n","        # df.rename_axis(names_map, axis=1, inplace=True) \n","        df.rename(columns=names_map, inplace=True) \n","        df.dropna(inplace=True)\n","        yield df\n","\n","store = pd.HDFStore(\"data/aqi/aqi.hdf5\", complib=\"blosc\", complevel=9)\n","string_size = {\"City\": 12, \"Position\": 30, \"Level\":12}\n","\n","for idx, df in enumerate(read_aqi_files(u\"data/aqi/*.csv\")):\n","    store.append('aqi', df, append=idx!=0, min_itemsize=string_size, data_columns=True) #❸\n","    \n","store.close()"]},{"cell_type":"markdown","metadata":{},"source":["下面開啟 aqi.hdf5 檔案並讀取所有資料："]},{"cell_type":"code","execution_count":12,"metadata":{"collapsed":false},"outputs":[{"name":"stdout","output_type":"stream","text":["337250\n"]}],"source":["store = pd.HDFStore(\"data/aqi/aqi.hdf5\")\n","df_aqi = store.select(\"aqi\")\n","print( len(df_aqi) )"]},{"cell_type":"markdown","metadata":{},"source":["下面唯讀取 PM2.5 值大於 500 的行："]},{"cell_type":"code","execution_count":13,"metadata":{"collapsed":false},"outputs":[{"name":"stdout","output_type":"stream","text":["87\n"]}],"source":["df_polluted = store.select(\"aqi\", where=\"PM2_5 > 500\")\n","print( len(df_polluted) )"]},{"cell_type":"markdown","metadata":{},"source":["### 讀寫資料庫\n","\n","用 `to_sql()` 可以將資料寫入 SQL 資料庫，它的第一個參數為資料庫的表名，第二個參數為表示與資料庫連接的 `Engine` 物件，`Engine` 在 `sqlalchemy` 函數庫中定義。下面首先從 `sqlalchemy` 中載入 `create_engine()` ，並呼叫它使用 SQLite 開啟資料庫檔案 \"data/aqi/aqi.db\" 。當該檔案不存在時，將建立新的資料庫檔案："]},{"cell_type":"code","execution_count":14,"metadata":{"collapsed":true},"outputs":[],"source":["from sqlalchemy import create_engine\n","engine = create_engine('sqlite:///data/aqi/aqi.db')"]},{"cell_type":"markdown","metadata":{},"source":["為了避免重複寫入，下面先透過 `engine` 物件執行 SQL 敘述，刪除 aqi 表："]},{"cell_type":"code","execution_count":15,"metadata":{"collapsed":true},"outputs":[],"source":["try:\n","    engine.execute(\"DROP TABLE aqi\")\n","except:\n","    pass"]},{"cell_type":"markdown","metadata":{},"source":["然後呼叫 `to_sql()` 將資料寫入資料庫，`if_exists` 參數為 `\"append\"` 表示當表存在時，將新資料增加到表中。由於本例中 `DataFrame` 物件的行索引無實際意義，因此設定 `index` 參數為 `False`，表示不儲存行索引。由於資料庫要求使用 Unicode 字串，因此在寫入資料庫之前對字串進行解碼，將其資料轉為 Unicode 字串。如果在從 CSV 檔案讀取資料時，透過 `encoding` 參數指定了文字編碼，則不必執行此步驟。"]},{"cell_type":"code","execution_count":16,"metadata":{"collapsed":false},"outputs":[],"source":["str_cols = [\"Position\", \"City\", \"Level\"]\n","\n","for df in read_aqi_files(\"data/aqi/*.csv\"):\n","    for col in str_cols:\n","        df[col] = df[col].str.decode(\"utf8\")\n","    df.to_sql(\"aqi\", engine, if_exists=\"append\", index=False)"]},{"cell_type":"markdown","metadata":{},"source":["下面呼叫 `read_sql()` 從資料庫讀取整數個名為 aqi 的表:"]},{"cell_type":"code","execution_count":17,"metadata":{"collapsed":false},"outputs":[],"source":["df_aqi = pd.read_sql(\"aqi\", engine)"]},{"cell_type":"markdown","metadata":{},"source":["也可以透過 SQL 查詢敘述讀取部分資料，下面只讀取 PM2.5 值大於 500 的行："]},{"cell_type":"code","execution_count":18,"metadata":{"collapsed":false},"outputs":[{"name":"stdout","output_type":"stream","text":["87\n"]}],"source":["df_polluted = pd.read_sql(\"select * from aqi where PM2_5 > 500\", engine)\n","print( len(df_polluted) )"]},{"cell_type":"markdown","metadata":{},"source":["### 使用Pickle序列化\n","\n","還可以使用 `to_pickle()` 和 `read_pickle()` 對 `DataFrame` 物件進行序列化和反序列化："]},{"cell_type":"code","execution_count":19,"metadata":{"collapsed":false},"outputs":[{"data":{"text/plain":["True"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["df_aqi.to_pickle(\"data/aqi/aqi.pickle\")\n","df_aqi2 = pd.read_pickle(\"data/aqi/aqi.pickle\")\n","df_aqi.equals(df_aqi2)"]},{"cell_type":"markdown","metadata":{},"source":["Pickle 是 python 特有的物件序列化格式，因此很難使用其他軟體、程式語言讀取 Pickle 化之後的資料，但是作為臨時儲存運算的中間結果還是很方便的。"]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"interpreter":{"hash":"355e075dd97f6b27be117f6c408f5915b9c46161d85f957be452b5de6ebf96d3"},"kernelspec":{"display_name":"Python 2","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"}},"nbformat":4,"nbformat_minor":0}
